import os
import tempfile
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, List, Optional, Tuple

import streamlit as st
from dotenv import load_dotenv
from langchain_text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.blob_loaders.youtube_audio import (
    YoutubeAudioLoader,
)
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import OpenAIWhisperParser
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, SystemMessage

load_dotenv()

try:
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
except ImportError as exc:  # pragma: no cover
    raise ImportError("langchain-openai íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.") from exc

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    ChatGoogleGenerativeAI = None  # type: ignore

try:
    from langchain_anthropic import ChatAnthropic
except ImportError:
    ChatAnthropic = None  # type: ignore

MODEL_OPTIONS = ["gpt-4o", "gpt-5", "gemini-2.5-pro", "claude-4-sonnet"]

PAGE_TITLE = "Youtube Q&A Chatbot"


def init_session_state() -> None:
    defaults = {
        "chat_history": [],
        "conversation_memory": [],
        "vectorstore": None,
        "retriever": None,
        "processed_url": None,
        "summary": None,
        "chunk_summaries": [],
        "is_processing": False,
        "selected_model": MODEL_OPTIONS[0],
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


def reset_state() -> None:
    keys_to_reset = [
        "chat_history",
        "conversation_memory",
        "vectorstore",
        "retriever",
        "processed_url",
        "summary",
        "chunk_summaries",
    ]
    for key in keys_to_reset:
        st.session_state[key] = [] if isinstance(st.session_state.get(key), list) else None
    st.session_state["is_processing"] = False


def get_openai_api_key() -> Optional[str]:
    return os.getenv("OPENAI_API_KEY")


def get_llm(model_name: str):
    if model_name in {"gpt-4o", "gpt-5"}:
        api_key = get_openai_api_key()
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return ChatOpenAI(model=model_name, temperature=0.2, api_key=api_key)

    if model_name == "gemini-2.5-pro":
        if ChatGoogleGenerativeAI is None:
            raise ImportError("langchain-google-genai íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY ë˜ëŠ” GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return ChatGoogleGenerativeAI(model="gemini-2.5-pro", google_api_key=api_key, temperature=0.2)

    if model_name == "claude-4-sonnet":
        if ChatAnthropic is None:
            raise ImportError("langchain-anthropic íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return ChatAnthropic(model="claude-4-sonnet", temperature=0.2, anthropic_api_key=api_key)

    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model_name}")


def summarize_chunk(chunk: str, model_name: str, chunk_idx: int) -> str:
    llm = get_llm(model_name)
    messages = [
        SystemMessage(
            content="ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì „ì‚¬ ë‚´ìš©ì„ ì •ëˆëœ ìš”ì•½ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì „ë¬¸ ë¹„ì„œì…ë‹ˆë‹¤. í•µì‹¬ ë©”ì‹œì§€ì™€ ì£¼ìš” ì‚¬ì‹¤ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”."
        ),
        HumanMessage(
            content=f"ë‹¤ìŒì€ ì „ì‚¬ ì²­í¬ #{chunk_idx + 1}ì…ë‹ˆë‹¤.\n\n{chunk}\n\nìœ„ ë‚´ìš©ì„ 5ë¬¸ì¥ ì´í•˜ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ê³ , í•µì‹¬ í‚¤ì›Œë“œ 3ê°œë¥¼ bullet ì—†ì´ í•œ ë¬¸ì¥ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”."
        ),
    ]
    response = llm.invoke(messages)
    return response.content.strip()


def aggregate_summary(chunk_summaries: List[str], model_name: str) -> str:
    llm = get_llm(model_name)
    summaries_text = "\n\n".join(f"[ì²­í¬ {idx + 1}] {summary}" for idx, summary in enumerate(chunk_summaries))
    messages = [
        SystemMessage(
            content="ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì˜ìƒ ë‚´ìš©ì„ ì „ì²´ì ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì²­ì¤‘ì—ê²Œ ì´í•´í•˜ê¸° ì‰¬ìš´ êµ¬ì¡°í™”ëœ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤."
        ),
        HumanMessage(
            content=(
                "ë‹¤ìŒì€ ìœ íŠœë¸Œ ì˜ìƒ ì „ì‚¬ ë‚´ìš©ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ìš”ì•½í•œ ê²°ê³¼ ì…ë‹ˆë‹¤.\n\n"
                f"{summaries_text}\n\n"
                "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ìƒ ì „ì²´ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”. í˜•ì‹ì€ ë‹¤ìŒì„ ë”°ë¦…ë‹ˆë‹¤:\n"
                "1. í•œ ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë©”ì‹œì§€ë¥¼ ì œì‹œí•˜ëŠ” 'í•µì‹¬ ìš”ì•½'\n"
                "2. ì£¼ìš” ë‚´ìš©ì„ 3~5ê°œ í•­ëª©ìœ¼ë¡œ ì •ë¦¬í•œ 'ì£¼ìš” ë‚´ìš©'\n"
                "3. ì‹œì²­ìê°€ ì–»ì„ ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë‚˜ ë‹¤ìŒ í–‰ë™ì„ ì œì•ˆí•˜ëŠ” 'í™œìš© í¬ì¸íŠ¸'\n"
                "ê° í•­ëª©ì€ ë¬¸ì¥í˜•ìœ¼ë¡œ ì‘ì„±í•˜ê³  ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”."
            )
        ),
    ]
    response = llm.invoke(messages)
    return response.content.strip()


def build_vector_store(chunks: List[str]) -> Tuple[FAISS, Any]:
    api_key = get_openai_api_key()
    if not api_key:
        raise ValueError("ì„ë² ë”© ìƒì„±ì„ ìœ„í•´ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    embeddings = OpenAIEmbeddings(api_key=api_key)
    vectorstore = FAISS.from_texts(chunks, embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 6})
    return vectorstore, retriever


def process_youtube_video(url: str, model_name: str) -> None:
    st.session_state["is_processing"] = True

    progress_text = st.sidebar.empty()
    progress_bar = st.sidebar.progress(0)

    try:
        api_key = get_openai_api_key()
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        progress_text.write("1/5 â–¶ ë™ì˜ìƒ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        with tempfile.TemporaryDirectory() as temp_dir:
            loader = GenericLoader(
                YoutubeAudioLoader([url], temp_dir),
                OpenAIWhisperParser(api_key=api_key),
            )
            docs = loader.load()
        progress_bar.progress(20)

        progress_text.write("2/5 â–¶ ì „ì‚¬ í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘...")
        combined_docs = [doc.page_content for doc in docs]
        transcript_text = "\n".join(combined_docs)

        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
        splits = splitter.split_text(transcript_text)
        if not splits:
            raise ValueError("ì „ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        progress_bar.progress(35)

        progress_text.write("3/5 â–¶ ì²­í¬ ìš”ì•½ ë³‘ë ¬ ìˆ˜í–‰ ì¤‘...")
        chunk_summaries: List[str] = [""] * len(splits)
        with ThreadPoolExecutor(max_workers=min(4, len(splits))) as executor:
            futures = {
                executor.submit(summarize_chunk, chunk, model_name, idx): idx
                for idx, chunk in enumerate(splits)
            }
            completed = 0
            total = len(futures)
            for future in as_completed(futures):
                idx = futures[future]
                try:
                    chunk_summaries[idx] = future.result()
                except Exception as err:  # pragma: no cover
                    chunk_summaries[idx] = f"ìš”ì•½ ì‹¤íŒ¨: {err}"
                completed += 1
                progress_bar.progress(35 + int(30 * completed / total))
                progress_text.write(f"3/5 â–¶ ì²­í¬ ìš”ì•½ ë³‘ë ¬ ìˆ˜í–‰ ì¤‘... ({completed}/{total})")

        progress_text.write("4/5 â–¶ ì „ì²´ ìš”ì•½ í†µí•© ì¤‘...")
        overall_summary = aggregate_summary(chunk_summaries, model_name)
        progress_bar.progress(75)

        progress_text.write("5/5 â–¶ QA ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        vectorstore, retriever = build_vector_store(splits)
        progress_bar.progress(100)

        st.session_state["vectorstore"] = vectorstore
        st.session_state["retriever"] = retriever
        st.session_state["processed_url"] = url
        st.session_state["summary"] = overall_summary
        st.session_state["chunk_summaries"] = chunk_summaries
        st.session_state["chat_history"] = []
        st.session_state["conversation_memory"] = []

        progress_text.success("ë™ì˜ìƒ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as error:
        reset_state()
        progress_text.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}")
    finally:
        st.session_state["is_processing"] = False


def answer_question(prompt: str, model_name: str) -> str:
    if st.session_state.get("retriever") is None:
        raise ValueError("ë¨¼ì € ë™ì˜ìƒì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")

    retrieved_docs = st.session_state["retriever"].invoke(prompt)
    if not retrieved_docs:
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?"

    context_text = ""
    for idx, doc in enumerate(retrieved_docs[:5], start=1):
        context_text += f"[ì°¸ê³  {idx}]\n{doc.page_content}\n\n"

    llm = get_llm(model_name)
    messages = [
        SystemMessage(
            content=(
                "ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì˜ìƒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆì˜ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                "ì œê³µëœ ì°¸ê³  ë¬¸ì„œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì •í™•í•˜ê³  ê·¼ê±° ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”."
            )
        ),
        HumanMessage(
            content=(
                f"ì§ˆë¬¸: {prompt}\n\n"
                f"ì°¸ê³  ë¬¸ì„œ:\n{context_text}\n"
                "ìœ„ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , í•„ìš”í•œ ê²½ìš° ìš”ì•½ëœ ì°¸ê³  ê·¼ê±°ë¥¼ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”."
            )
        ),
    ]
    response = llm.invoke(messages)
    return response.content.strip()


def render_css() -> None:
    st.markdown(
        """
<style>
.main-title {
    text-align: center;
    font-size: 2.4rem;
    font-weight: 700;
    margin-bottom: 0.5rem;
    color: #1f77b4;
}
.summary-box {
    background-color: #f8f9fa;
    border: 1px solid #e1e4e8;
    border-radius: 8px;
    padding: 1.5rem;
    margin-bottom: 1.5rem;
}
.chunk-expander > div {
    border: 1px solid #e1e4e8 !important;
    border-radius: 6px !important;
    margin-bottom: 0.5rem !important;
}
.stChatMessage {
    font-size: 0.95rem;
}
</style>
        """,
        unsafe_allow_html=True,
    )


def main() -> None:
    st.set_page_config(page_title=PAGE_TITLE, page_icon="ğŸ¬", layout="wide")
    render_css()
    init_session_state()

    with st.sidebar:
        st.markdown("#### ëª¨ë¸ ì„ íƒ")
        selected_model = st.selectbox(
            "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", MODEL_OPTIONS, index=MODEL_OPTIONS.index(st.session_state["selected_model"])
        )
        st.session_state["selected_model"] = selected_model

        st.markdown("#### YouTube URL")
        youtube_url = st.text_input("ë™ì˜ìƒ URLì„ ì…ë ¥í•˜ì„¸ìš”", value=st.session_state.get("processed_url") or "")

        process_disabled = st.session_state["is_processing"] or not youtube_url

        if st.button("ë™ì˜ìƒ ì²˜ë¦¬í•˜ê¸°", disabled=process_disabled):
            reset_state()
            process_youtube_video(youtube_url, selected_model)

        st.button("ë‹¤ì‹œ ì‹œì‘í•˜ê¸°", on_click=reset_state, type="secondary")

    st.markdown(f"<div class='main-title'>{PAGE_TITLE}</div>", unsafe_allow_html=True)

    if st.session_state.get("summary"):
        st.markdown("### ì˜ìƒ ìš”ì•½")
        st.markdown(f"<div class='summary-box'>{st.session_state['summary']}</div>", unsafe_allow_html=True)
        with st.expander("ì²­í¬ë³„ ìš”ì•½ ë³´ê¸°"):
            for idx, chunk_summary in enumerate(st.session_state.get("chunk_summaries", []), start=1):
                st.markdown(f"**ì²­í¬ {idx} ìš”ì•½**")
                st.write(chunk_summary)

    if st.session_state["chat_history"]:
        for message in st.session_state["chat_history"]:
            with st.chat_message(message["role"]):
                st.write(message["content"])

    if user_prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ìƒ ì²˜ë¦¬ í›„ ì‚¬ìš© ê°€ëŠ¥)"):
        st.session_state["chat_history"].append({"role": "user", "content": user_prompt})
        with st.chat_message("user"):
            st.write(user_prompt)

        try:
            answer = answer_question(user_prompt, st.session_state["selected_model"])
        except Exception as error:
            answer = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}"

        with st.chat_message("assistant"):
            st.write(answer)

        st.session_state["chat_history"].append({"role": "assistant", "content": answer})
        st.session_state["conversation_memory"].append(f"ì‚¬ìš©ì: {user_prompt}")
        st.session_state["conversation_memory"].append(f"AI: {answer}")


if __name__ == "__main__":
    main()

